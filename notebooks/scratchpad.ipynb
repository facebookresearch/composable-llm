{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from cot.data import Parity, BinaryCopy, Copy\n",
    "from cot.config import RAW_DIR\n",
    "from cot.models import Transformer, TransformerConfig\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem = BinaryCopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "seq_length = 20\n",
    "max_nb_data_per_len = 1000\n",
    "random = False\n",
    "split_probas_by_len =  [1, 1, 1, 1, .9, .8, .9, .3, .2]\n",
    "probas_by_len = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1]).astype(float)\n",
    "probas_by_len /= probas_by_len.sum()\n",
    "\n",
    "lengths = list(np.arange(len(split_probas_by_len)) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cot.data.data_processing:Sequences of length 1 done. Saved in /home/vivc/Code/llm/Compositionality/data/raw/binary_copy (2/2 split).\n",
      "INFO:cot.data.data_processing:Sequences of length 2 done. Saved in /home/vivc/Code/llm/Compositionality/data/raw/binary_copy (4/4 split).\n",
      "INFO:cot.data.data_processing:Sequences of length 3 done. Saved in /home/vivc/Code/llm/Compositionality/data/raw/binary_copy (8/8 split).\n",
      "INFO:cot.data.data_processing:Sequences of length 4 done. Saved in /home/vivc/Code/llm/Compositionality/data/raw/binary_copy (16/16 split).\n",
      "INFO:cot.data.data_processing:Sequences of length 5 done. Saved in /home/vivc/Code/llm/Compositionality/data/raw/binary_copy (28/32 split).\n",
      "INFO:cot.data.data_processing:Sequences of length 6 done. Saved in /home/vivc/Code/llm/Compositionality/data/raw/binary_copy (51/64 split).\n",
      "INFO:cot.data.data_processing:Sequences of length 7 done. Saved in /home/vivc/Code/llm/Compositionality/data/raw/binary_copy (115/128 split).\n",
      "INFO:cot.data.data_processing:Sequences of length 8 done. Saved in /home/vivc/Code/llm/Compositionality/data/raw/binary_copy (76/256 split).\n",
      "INFO:cot.data.data_processing:Sequences of length 9 done. Saved in /home/vivc/Code/llm/Compositionality/data/raw/binary_copy (102/512 split).\n",
      "INFO:cot.data.data_processing:Loading training data.\n",
      "INFO:cot.data.data_processing:Setting sampler.\n",
      "INFO:cot.data.data_processing:Loading test data for binary_copy problem.\n"
     ]
    }
   ],
   "source": [
    "if Problem.prefix == 'copy':\n",
    "    Problem(vocab_size=20)\n",
    "\n",
    "Problem.generate_datafiles(max_nb_data_per_len, split_probas_by_len, rng)\n",
    "\n",
    "trainset = Problem()\n",
    "trainset.set_as_trainset(lengths, probas_by_len)\n",
    "\n",
    "testset = Problem()\n",
    "testset.set_as_testset(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(trainset, batch_size=32, sampler=trainset.sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[1].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TransformerConfig(\n",
    "    vocab_size=4,\n",
    "    emb_dim=128,\n",
    "    pos_emb=True,\n",
    "    seq_len=20,\n",
    "    emb_dropout=0.1,\n",
    "    n_head=2,\n",
    "    n_layer=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (embeddings): Embedding(\n",
      "    (token_emb): Embedding(4, 128)\n",
      "    (pos_emb): Embedding(20, 128)\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0-1): 2 x TransformerBlock(\n",
      "      (norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): SelfAttention(\n",
      "        (qkv_mat): Linear(in_features=128, out_features=384, bias=False)\n",
      "        (output): Linear(in_features=128, out_features=128, bias=False)\n",
      "      )\n",
      "      (norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (fc1): Linear(in_features=128, out_features=512, bias=False)\n",
      "        (fc2): Linear(in_features=512, out_features=128, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (output): Linear(in_features=128, out_features=4, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7.6854, Accuracy: 0.0100\n",
      "Loss: 4.7039, Accuracy: 0.1244\n",
      "Loss: 3.6002, Accuracy: 0.2587\n",
      "Loss: 2.6853, Accuracy: 0.3607\n",
      "Loss: 2.0849, Accuracy: 0.4204\n",
      "Loss: 1.9623, Accuracy: 0.4279\n",
      "Loss: 1.7897, Accuracy: 0.4726\n",
      "Loss: 1.5350, Accuracy: 0.5075\n",
      "Loss: 1.4144, Accuracy: 0.5348\n",
      "Loss: 1.2897, Accuracy: 0.5945\n",
      "Loss: 0.9764, Accuracy: 0.6493\n",
      "Loss: 0.6903, Accuracy: 0.7662\n",
      "Loss: 0.5577, Accuracy: 0.7985\n",
      "Loss: 0.4651, Accuracy: 0.8209\n",
      "Loss: 0.4739, Accuracy: 0.8184\n",
      "Loss: 0.4335, Accuracy: 0.8557\n",
      "Loss: 0.3097, Accuracy: 0.8955\n",
      "Loss: 0.2448, Accuracy: 0.9254\n",
      "Loss: 0.2171, Accuracy: 0.9204\n",
      "Loss: 0.2344, Accuracy: 0.9229\n",
      "Loss: 0.1915, Accuracy: 0.9353\n",
      "Loss: 0.1941, Accuracy: 0.9428\n",
      "Loss: 0.1926, Accuracy: 0.9353\n",
      "Loss: 0.1018, Accuracy: 0.9751\n",
      "Loss: 0.1010, Accuracy: 0.9652\n",
      "Loss: 0.0892, Accuracy: 0.9751\n",
      "Loss: 0.0803, Accuracy: 0.9801\n",
      "Loss: 0.0764, Accuracy: 0.9776\n",
      "Loss: 0.0685, Accuracy: 0.9876\n",
      "Loss: 0.1012, Accuracy: 0.9677\n",
      "Loss: 0.0776, Accuracy: 0.9751\n",
      "Loss: 0.0648, Accuracy: 0.9801\n",
      "Loss: 0.0445, Accuracy: 0.9876\n",
      "Loss: 0.0871, Accuracy: 0.9726\n",
      "Loss: 0.0984, Accuracy: 0.9701\n",
      "Loss: 0.0620, Accuracy: 0.9826\n",
      "Loss: 0.0712, Accuracy: 0.9776\n",
      "Loss: 0.0611, Accuracy: 0.9776\n",
      "Loss: 0.0645, Accuracy: 0.9776\n",
      "Loss: 0.0704, Accuracy: 0.9876\n",
      "Loss: 0.0595, Accuracy: 0.9876\n",
      "Loss: 0.0649, Accuracy: 0.9851\n",
      "Loss: 0.0650, Accuracy: 0.9776\n",
      "Loss: 0.0641, Accuracy: 0.9751\n",
      "Loss: 0.0527, Accuracy: 0.9876\n",
      "Loss: 0.0709, Accuracy: 0.9776\n",
      "Loss: 0.0351, Accuracy: 0.9900\n",
      "Loss: 0.0472, Accuracy: 0.9925\n",
      "Loss: 0.0392, Accuracy: 0.9900\n",
      "Loss: 0.0963, Accuracy: 0.9726\n",
      "Loss: 0.0458, Accuracy: 0.9900\n",
      "Loss: 0.0767, Accuracy: 0.9776\n",
      "Loss: 0.0540, Accuracy: 0.9900\n",
      "Loss: 0.0631, Accuracy: 0.9776\n",
      "Loss: 0.0341, Accuracy: 0.9925\n",
      "Loss: 0.0454, Accuracy: 0.9925\n",
      "Loss: 0.0649, Accuracy: 0.9751\n",
      "Loss: 0.0603, Accuracy: 0.9801\n",
      "Loss: 0.0337, Accuracy: 0.9900\n",
      "Loss: 0.0484, Accuracy: 0.9900\n",
      "Loss: 0.0765, Accuracy: 0.9826\n",
      "Loss: 0.0765, Accuracy: 0.9751\n",
      "Loss: 0.0424, Accuracy: 0.9876\n",
      "Loss: 0.0568, Accuracy: 0.9851\n",
      "Loss: 0.0542, Accuracy: 0.9900\n",
      "Loss: 0.0661, Accuracy: 0.9876\n",
      "Loss: 0.0365, Accuracy: 0.9925\n",
      "Loss: 0.0788, Accuracy: 0.9776\n",
      "Loss: 0.0368, Accuracy: 0.9851\n",
      "Loss: 0.0373, Accuracy: 0.9876\n",
      "Loss: 0.0597, Accuracy: 0.9900\n",
      "Loss: 0.0599, Accuracy: 0.9851\n",
      "Loss: 0.0600, Accuracy: 0.9851\n",
      "Loss: 0.0695, Accuracy: 0.9851\n",
      "Loss: 0.0553, Accuracy: 0.9851\n",
      "Loss: 0.0707, Accuracy: 0.9776\n",
      "Loss: 0.0605, Accuracy: 0.9851\n",
      "Loss: 0.0624, Accuracy: 0.9801\n",
      "Loss: 0.0586, Accuracy: 0.9900\n",
      "Loss: 0.0583, Accuracy: 0.9826\n",
      "Loss: 0.0412, Accuracy: 0.9900\n",
      "Loss: 0.0636, Accuracy: 0.9801\n",
      "Loss: 0.0435, Accuracy: 0.9925\n",
      "Loss: 0.0435, Accuracy: 0.9950\n",
      "Loss: 0.0334, Accuracy: 0.9975\n",
      "Loss: 0.0495, Accuracy: 0.9900\n",
      "Loss: 0.0522, Accuracy: 0.9801\n",
      "Loss: 0.0674, Accuracy: 0.9801\n",
      "Loss: 0.0790, Accuracy: 0.9751\n",
      "Loss: 0.0704, Accuracy: 0.9826\n",
      "Loss: 0.0651, Accuracy: 0.9851\n",
      "Loss: 0.0602, Accuracy: 0.9900\n",
      "Loss: 0.0630, Accuracy: 0.9900\n",
      "Loss: 0.0370, Accuracy: 0.9900\n",
      "Loss: 0.0633, Accuracy: 0.9826\n",
      "Loss: 0.0474, Accuracy: 0.9876\n",
      "Loss: 0.0476, Accuracy: 0.9826\n",
      "Loss: 0.0637, Accuracy: 0.9876\n",
      "Loss: 0.0455, Accuracy: 0.9876\n",
      "Loss: 0.0434, Accuracy: 0.9876\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Transformer(config)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "\n",
    "nb_epochs = 100\n",
    "\n",
    "losses = np.empty(nb_epochs)\n",
    "accuracies = np.empty(nb_epochs)\n",
    "test_losses = []\n",
    "\n",
    "model.train()\n",
    "for t in range(nb_epochs):\n",
    "    running_loss = 0\n",
    "    accuracy = 0\n",
    "    for sequence in loader:\n",
    "        # deal with EoS token being represented as -1\n",
    "        sequence += 1\n",
    "        sequence = sequence.to(device=device, dtype=torch.long)\n",
    "\n",
    "        inputs = sequence[:, :-1]\n",
    "        targets = sequence[:, 1:]\n",
    "\n",
    "        # only train on the chain-of-thoughts process, EoI is represented by 3 in our case\n",
    "        ind = targets == 3\n",
    "        cot_mask = ind.cumsum(axis=1)\n",
    "        cot_mask[ind] = 0\n",
    "        cot_mask = cot_mask.to(dtype=bool)\n",
    "\n",
    "        logits = model(inputs)\n",
    "        loss = F.cross_entropy(logits[cot_mask].view(-1, logits.size(-1)), targets[cot_mask].reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            running_loss += loss.item()\n",
    "            tmp = (logits.argmax(dim=-1) - targets)\n",
    "            tmp[~cot_mask] = 0\n",
    "            accuracy += (tmp == 0).all(dim=1).float().sum()\n",
    "\n",
    "    accuracy /= len(trainset)\n",
    "\n",
    "    accuracies[t] = accuracy\n",
    "    losses[t] = loss\n",
    "    print(f'Loss: {running_loss:.4f}, Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "\n",
    "There are several quantities to monitor:\n",
    "- Make sure that `-1` is an absorbing state.\n",
    "- Check the validity of the full chain of thoughts.\n",
    "- Check the validity of the final answer in the chain, or of intermediate answer in the chain.\n",
    "- Cluster results by lengths of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
