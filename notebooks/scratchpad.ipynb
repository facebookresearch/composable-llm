{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from cot.data import Parity, BinaryCopy, Copy\n",
    "from cot.config import RAW_DIR\n",
    "from cot.models import Transformer, TransformerConfig\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem = Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "seq_length = 20\n",
    "max_nb_data_per_len = 1000\n",
    "random = False\n",
    "split_probas_by_len =  [1, 1, 1, 1, .9, .8, .9, .3, .2]\n",
    "probas_by_len = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1]).astype(float)\n",
    "probas_by_len /= probas_by_len.sum()\n",
    "\n",
    "lengths = list(np.arange(len(split_probas_by_len)) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cot.data.data_processing:Sequences of length 1 done. Saved in /home/vivc/Code/llm/Compositionality/data/raw/parity (2/2 split).\n",
      "INFO:cot.data.data_processing:Sequences of length 2 done. Saved in /home/vivc/Code/llm/Compositionality/data/raw/parity (4/4 split).\n",
      "INFO:cot.data.data_processing:Sequences of length 3 done. Saved in /home/vivc/Code/llm/Compositionality/data/raw/parity (8/8 split).\n",
      "INFO:cot.data.data_processing:Sequences of length 4 done. Saved in /home/vivc/Code/llm/Compositionality/data/raw/parity (16/16 split).\n",
      "INFO:cot.data.data_processing:Sequences of length 5 done. Saved in /home/vivc/Code/llm/Compositionality/data/raw/parity (28/32 split).\n",
      "INFO:cot.data.data_processing:Sequences of length 6 done. Saved in /home/vivc/Code/llm/Compositionality/data/raw/parity (51/64 split).\n",
      "INFO:cot.data.data_processing:Sequences of length 7 done. Saved in /home/vivc/Code/llm/Compositionality/data/raw/parity (115/128 split).\n",
      "INFO:cot.data.data_processing:Sequences of length 8 done. Saved in /home/vivc/Code/llm/Compositionality/data/raw/parity (76/256 split).\n",
      "INFO:cot.data.data_processing:Sequences of length 9 done. Saved in /home/vivc/Code/llm/Compositionality/data/raw/parity (102/512 split).\n",
      "INFO:cot.data.data_processing:Loading training data.\n",
      "INFO:cot.data.data_processing:Setting sampler.\n",
      "INFO:cot.data.data_processing:Loading test data for parity problem.\n"
     ]
    }
   ],
   "source": [
    "if Problem.prefix == 'copy':\n",
    "    Problem(vocab_size=20)\n",
    "\n",
    "Problem.generate_datafiles(max_nb_data_per_len, split_probas_by_len, rng)\n",
    "\n",
    "trainset = Problem()\n",
    "trainset.set_as_trainset(lengths, probas_by_len)\n",
    "\n",
    "testset = Problem()\n",
    "testset.set_as_testset(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(trainset, batch_size=256, sampler=trainset.sampler)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TransformerConfig(\n",
    "    vocab_size=torch.max(trainset.data).item() + 1,\n",
    "    emb_dim=128,\n",
    "    pos_emb=True,\n",
    "    seq_len=len(trainset[0]),\n",
    "    emb_dropout=0.1,\n",
    "    n_head=2,\n",
    "    n_layer=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (embeddings): Embedding(\n",
      "    (token_emb): Embedding(5, 128)\n",
      "    (pos_emb): Embedding(21, 128)\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0-1): 2 x TransformerBlock(\n",
      "      (norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): SelfAttention(\n",
      "        (qkv_mat): Linear(in_features=128, out_features=384, bias=False)\n",
      "        (output): Linear(in_features=128, out_features=128, bias=False)\n",
      "      )\n",
      "      (norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (fc1): Linear(in_features=128, out_features=512, bias=False)\n",
      "        (fc2): Linear(in_features=512, out_features=128, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (output): Linear(in_features=128, out_features=5, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vivc/Code/conda/envs/llm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.4651, Accuracy: 0.0050\n",
      "Loss: 1.2965, Accuracy: 0.0124\n",
      "Loss: 0.9876, Accuracy: 0.0050\n",
      "Loss: 0.9343, Accuracy: 0.0000\n",
      "Loss: 0.8938, Accuracy: 0.0000\n",
      "Loss: 0.8772, Accuracy: 0.0000\n",
      "Loss: 0.8131, Accuracy: 0.0522\n",
      "Loss: 0.8033, Accuracy: 0.0871\n",
      "Loss: 0.7537, Accuracy: 0.0821\n",
      "Loss: 0.7520, Accuracy: 0.0995\n",
      "Loss: 0.7314, Accuracy: 0.1741\n",
      "Loss: 0.7091, Accuracy: 0.1667\n",
      "Loss: 0.6855, Accuracy: 0.1791\n",
      "Loss: 0.6968, Accuracy: 0.1368\n",
      "Loss: 0.6702, Accuracy: 0.1716\n",
      "Loss: 0.6580, Accuracy: 0.1866\n",
      "Loss: 0.6054, Accuracy: 0.1965\n",
      "Loss: 0.6208, Accuracy: 0.1841\n",
      "Loss: 0.5773, Accuracy: 0.1965\n",
      "Loss: 0.5381, Accuracy: 0.2289\n",
      "Loss: 0.5025, Accuracy: 0.2512\n",
      "Loss: 0.4681, Accuracy: 0.2612\n",
      "Loss: 0.4385, Accuracy: 0.2786\n",
      "Loss: 0.4216, Accuracy: 0.2861\n",
      "Loss: 0.4177, Accuracy: 0.3010\n",
      "Loss: 0.4084, Accuracy: 0.2786\n",
      "Loss: 0.3955, Accuracy: 0.3234\n",
      "Loss: 0.3902, Accuracy: 0.3284\n",
      "Loss: 0.3451, Accuracy: 0.3657\n",
      "Loss: 0.3689, Accuracy: 0.3507\n",
      "Loss: 0.3312, Accuracy: 0.3682\n",
      "Loss: 0.3353, Accuracy: 0.3930\n",
      "Loss: 0.3110, Accuracy: 0.4378\n",
      "Loss: 0.3267, Accuracy: 0.3980\n",
      "Loss: 0.3613, Accuracy: 0.3632\n",
      "Loss: 0.3544, Accuracy: 0.3781\n",
      "Loss: 0.2934, Accuracy: 0.4453\n",
      "Loss: 0.3432, Accuracy: 0.3781\n",
      "Loss: 0.3273, Accuracy: 0.3881\n",
      "Loss: 0.2993, Accuracy: 0.4179\n",
      "Loss: 0.3142, Accuracy: 0.4403\n",
      "Loss: 0.3178, Accuracy: 0.3955\n",
      "Loss: 0.3273, Accuracy: 0.4080\n",
      "Loss: 0.3368, Accuracy: 0.3980\n",
      "Loss: 0.3195, Accuracy: 0.4005\n",
      "Loss: 0.2996, Accuracy: 0.4080\n",
      "Loss: 0.3207, Accuracy: 0.4104\n",
      "Loss: 0.3142, Accuracy: 0.4229\n",
      "Loss: 0.2885, Accuracy: 0.4204\n",
      "Loss: 0.2683, Accuracy: 0.4627\n",
      "Loss: 0.2992, Accuracy: 0.4428\n",
      "Loss: 0.3263, Accuracy: 0.4030\n",
      "Loss: 0.3130, Accuracy: 0.4204\n",
      "Loss: 0.2999, Accuracy: 0.4677\n",
      "Loss: 0.2467, Accuracy: 0.5000\n",
      "Loss: 0.2914, Accuracy: 0.4602\n",
      "Loss: 0.2948, Accuracy: 0.4328\n",
      "Loss: 0.2565, Accuracy: 0.4701\n",
      "Loss: 0.2933, Accuracy: 0.4403\n",
      "Loss: 0.2779, Accuracy: 0.4353\n",
      "Loss: 0.2889, Accuracy: 0.4627\n",
      "Loss: 0.2836, Accuracy: 0.4279\n",
      "Loss: 0.2704, Accuracy: 0.4851\n",
      "Loss: 0.2809, Accuracy: 0.4751\n",
      "Loss: 0.3165, Accuracy: 0.4403\n",
      "Loss: 0.2895, Accuracy: 0.4552\n",
      "Loss: 0.2997, Accuracy: 0.4279\n",
      "Loss: 0.2565, Accuracy: 0.4950\n",
      "Loss: 0.2848, Accuracy: 0.4378\n",
      "Loss: 0.2730, Accuracy: 0.4701\n",
      "Loss: 0.3075, Accuracy: 0.4428\n",
      "Loss: 0.2606, Accuracy: 0.5050\n",
      "Loss: 0.2643, Accuracy: 0.4776\n",
      "Loss: 0.2860, Accuracy: 0.4403\n",
      "Loss: 0.2726, Accuracy: 0.4552\n",
      "Loss: 0.2556, Accuracy: 0.5149\n",
      "Loss: 0.2531, Accuracy: 0.5174\n",
      "Loss: 0.2614, Accuracy: 0.4726\n",
      "Loss: 0.2709, Accuracy: 0.4577\n",
      "Loss: 0.2856, Accuracy: 0.4900\n",
      "Loss: 0.2470, Accuracy: 0.4925\n",
      "Loss: 0.2493, Accuracy: 0.4950\n",
      "Loss: 0.2342, Accuracy: 0.4900\n",
      "Loss: 0.2597, Accuracy: 0.4900\n",
      "Loss: 0.2589, Accuracy: 0.4876\n",
      "Loss: 0.2476, Accuracy: 0.4876\n",
      "Loss: 0.2826, Accuracy: 0.4577\n",
      "Loss: 0.2773, Accuracy: 0.4502\n",
      "Loss: 0.2691, Accuracy: 0.5050\n",
      "Loss: 0.2372, Accuracy: 0.5423\n",
      "Loss: 0.2719, Accuracy: 0.4652\n",
      "Loss: 0.2390, Accuracy: 0.5050\n",
      "Loss: 0.2505, Accuracy: 0.5025\n",
      "Loss: 0.2715, Accuracy: 0.5100\n",
      "Loss: 0.2388, Accuracy: 0.5249\n",
      "Loss: 0.2414, Accuracy: 0.5423\n",
      "Loss: 0.2357, Accuracy: 0.5348\n",
      "Loss: 0.2567, Accuracy: 0.4950\n",
      "Loss: 0.2490, Accuracy: 0.4701\n",
      "Loss: 0.2443, Accuracy: 0.5124\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Transformer(config)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "\n",
    "nb_epochs = 100\n",
    "\n",
    "losses = np.empty(nb_epochs)\n",
    "accuracies = np.empty(nb_epochs)\n",
    "test_losses = []\n",
    "\n",
    "model.train()\n",
    "for t in range(nb_epochs):\n",
    "    running_loss = 0\n",
    "    accuracy = 0\n",
    "    for sequence in loader:\n",
    "        sequence = sequence.to(device=device, dtype=torch.long)\n",
    "\n",
    "        inputs = sequence[:, :-1]\n",
    "        targets = sequence[:, 1:]\n",
    "\n",
    "        # only train on the chain-of-thoughts process, EoI is represented by 1 in our case\n",
    "        ind = targets == 1\n",
    "        cot_mask = ind.cumsum(axis=1)\n",
    "        cot_mask[ind] = 0\n",
    "        cot_mask = cot_mask.to(dtype=bool)\n",
    "\n",
    "        logits = model(inputs)\n",
    "        loss = F.cross_entropy(logits[cot_mask].view(-1, logits.size(-1)), targets[cot_mask].reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            running_loss += loss.item()\n",
    "            tmp = (logits.argmax(dim=-1) - targets)\n",
    "            tmp[~cot_mask] = 0\n",
    "            accuracy += (tmp == 0).all(dim=1).float().sum()\n",
    "\n",
    "    accuracy /= len(trainset)\n",
    "\n",
    "    accuracies[t] = accuracy\n",
    "    losses[t] = loss\n",
    "    print(f'Loss: {running_loss:.4f}, Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "\n",
    "There are several quantities to monitor:\n",
    "- Make sure that `-1` is an absorbing state.\n",
    "- Check the validity of the full chain of thoughts.\n",
    "- Check the validity of the final answer in the chain, or of intermediate answer in the chain.\n",
    "- Cluster results by lengths of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
