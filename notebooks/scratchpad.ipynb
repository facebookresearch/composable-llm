{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from cot.data import Parity, BinaryCopy, Copy\n",
    "from cot.config import RAW_DIR\n",
    "from cot.models import Transformer, TransformerConfig\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem = BinaryCopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "seq_length = 20\n",
    "max_nb_data_per_len = 1000\n",
    "random = False\n",
    "split_probas_by_len =  [1, 1, 1, 1, .9, .8, .9, .3, .2]\n",
    "probas_by_len = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1]).astype(float)\n",
    "probas_by_len /= probas_by_len.sum()\n",
    "\n",
    "lengths = list(np.arange(len(split_probas_by_len)) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cot.data.data_processing:Sequences of length 1 done. Saved in /home/vivc/Code/llm/Compositionality/data/raw/binary_copy (2/2 split).\n",
      "INFO:cot.data.data_processing:Sequences of length 2 done. Saved in /home/vivc/Code/llm/Compositionality/data/raw/binary_copy (4/4 split).\n",
      "INFO:cot.data.data_processing:Sequences of length 3 done. Saved in /home/vivc/Code/llm/Compositionality/data/raw/binary_copy (8/8 split).\n",
      "INFO:cot.data.data_processing:Sequences of length 4 done. Saved in /home/vivc/Code/llm/Compositionality/data/raw/binary_copy (16/16 split).\n",
      "INFO:cot.data.data_processing:Sequences of length 5 done. Saved in /home/vivc/Code/llm/Compositionality/data/raw/binary_copy (28/32 split).\n",
      "INFO:cot.data.data_processing:Sequences of length 6 done. Saved in /home/vivc/Code/llm/Compositionality/data/raw/binary_copy (51/64 split).\n",
      "INFO:cot.data.data_processing:Sequences of length 7 done. Saved in /home/vivc/Code/llm/Compositionality/data/raw/binary_copy (115/128 split).\n",
      "INFO:cot.data.data_processing:Sequences of length 8 done. Saved in /home/vivc/Code/llm/Compositionality/data/raw/binary_copy (76/256 split).\n",
      "INFO:cot.data.data_processing:Sequences of length 9 done. Saved in /home/vivc/Code/llm/Compositionality/data/raw/binary_copy (102/512 split).\n",
      "INFO:cot.data.data_processing:Loading training data.\n",
      "INFO:cot.data.data_processing:Setting sampler.\n",
      "INFO:cot.data.data_processing:Loading test data for binary_copy problem.\n"
     ]
    }
   ],
   "source": [
    "if Problem.prefix == 'copy':\n",
    "    Problem(vocab_size=20)\n",
    "\n",
    "Problem.generate_datafiles(max_nb_data_per_len, split_probas_by_len, rng)\n",
    "\n",
    "trainset = Problem()\n",
    "trainset.set_as_trainset(lengths, probas_by_len)\n",
    "\n",
    "testset = Problem()\n",
    "testset.set_as_testset(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(trainset, batch_size=32, sampler=trainset.sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[1].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TransformerConfig(\n",
    "    vocab_size=4,\n",
    "    emb_dim=128,\n",
    "    pos_emb=True,\n",
    "    seq_len=20,\n",
    "    emb_dropout=0.1,\n",
    "    n_head=2,\n",
    "    n_layer=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (embeddings): Embedding(\n",
      "    (token_emb): Embedding(4, 128)\n",
      "    (pos_emb): Embedding(20, 128)\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0-1): 2 x TransformerBlock(\n",
      "      (norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): SelfAttention(\n",
      "        (qkv_mat): Linear(in_features=128, out_features=384, bias=False)\n",
      "        (output): Linear(in_features=128, out_features=128, bias=False)\n",
      "      )\n",
      "      (norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (fc1): Linear(in_features=128, out_features=512, bias=False)\n",
      "        (fc2): Linear(in_features=512, out_features=128, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (output): Linear(in_features=128, out_features=4, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7.685434967279434\n",
      "Loss: 4.7039487063884735\n",
      "Loss: 3.600172221660614\n",
      "Loss: 2.6852837651968002\n",
      "Loss: 2.084871418774128\n",
      "Loss: 1.9622856751084328\n",
      "Loss: 1.7896827161312103\n",
      "Loss: 1.5350349992513657\n",
      "Loss: 1.414369598031044\n",
      "Loss: 1.2896583676338196\n",
      "Loss: 0.9764289855957031\n",
      "Loss: 0.6902799997478724\n",
      "Loss: 0.5577131491154432\n",
      "Loss: 0.46510985493659973\n",
      "Loss: 0.4739277195185423\n",
      "Loss: 0.43351088277995586\n",
      "Loss: 0.30967373214662075\n",
      "Loss: 0.24477957095950842\n",
      "Loss: 0.21711145667359233\n",
      "Loss: 0.23435441683977842\n",
      "Loss: 0.19153718138113618\n",
      "Loss: 0.19407782796770334\n",
      "Loss: 0.19256279850378633\n",
      "Loss: 0.10184032842516899\n",
      "Loss: 0.1010023639537394\n",
      "Loss: 0.08920602547004819\n",
      "Loss: 0.08030583639629185\n",
      "Loss: 0.07640748959966004\n",
      "Loss: 0.0685235713608563\n",
      "Loss: 0.10115879005752504\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Transformer(config)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "\n",
    "losses = []\n",
    "test_losses = []\n",
    "\n",
    "n_epochs = 30\n",
    "model.train()\n",
    "for _ in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for sequence in loader:\n",
    "        # deal with EoS token being represented as -1\n",
    "        sequence += 1\n",
    "        sequence = sequence.to(device=device, dtype=torch.long)\n",
    "\n",
    "        inputs = sequence[:, :-1]\n",
    "        targets = sequence[:, 1:]\n",
    "\n",
    "        # only train on the chain-of-thoughts process\n",
    "        ind = targets == 3\n",
    "        cot_mask = ind.cumsum(axis=1)\n",
    "        cot_mask[ind] = 0\n",
    "        cot_mask = cot_mask.to(dtype=bool)\n",
    "\n",
    "        logits = model(inputs)\n",
    "        loss = F.cross_entropy(logits[cot_mask].view(-1, logits.size(-1)), targets[cot_mask].reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    losses.append(total_loss)\n",
    "    print(f'Loss: {total_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "\n",
    "There are several quantities to monitor:\n",
    "- Make sure that `-1` is an absorbing state.\n",
    "- Check the validity of the full chain of thoughts.\n",
    "- Check the validity of the final answer in the chain, or of intermediate answer in the chain.\n",
    "- Cluster results by lengths of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(targets - logits.argmax(dim=-1))[cot_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
