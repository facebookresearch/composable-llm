"""
Generate synthetic data to study LLM behaviors in controlled settings.

The sequences contain the following special tokens:
    token 0: begining of sentence,
    token 1: end of input,
    token 2: end of sentence.
"""

import logging

import numpy as np
import torch
from torch.utils.data import Dataset, WeightedRandomSampler

from cot.config import RAW_DIR

logger = logging.getLogger(__name__)


# -----------------------------------------------------------------------------
# Generic class
# -----------------------------------------------------------------------------


class SequenceDataset(Dataset):
    data_dir = RAW_DIR
    prefix = None

    def __init__(self):
        pass

    @classmethod
    def generate_fixed_len_data(cls, seq_len, nb_data, rng=None):
        """Generate sequence with fixed sequence length."""
        raise NotImplementedError

    @classmethod
    def get_len(cls, seq_len):
        """Full sequence length."""
        raise NotImplementedError

    @classmethod
    def generate_datafiles(cls, max_nb_data_per_len, split_probas_by_len, rng=None):
        """
        Test/train split.

        Parameters
        ----------
        max_nb_data_per_len : int
            Maximum number of data points to generate for each sequence length.
        split_probas_by_len : list of float
            Proportion of data to put in the training set for each sequence length.
        rng : numpy.random.Generator, optional
            Random number generator. If None, use the default generator.
        """

        logger.info(f"Generating data. Saving in {cls.data_dir}")

        if rng is None:
            rng = np.random.default_rng()

        cls.data_dir.mkdir(parents=True, exist_ok=True)
        for seq_len, split_proba in enumerate(split_probas_by_len):
            seq_len += 1
            data = cls.generate_fixed_len_data(seq_len=seq_len, nb_data=max_nb_data_per_len, rng=rng)
            np.save(cls.data_dir / f"data_{seq_len}.npy", data)
            rng.shuffle(data)
            nb_train = int(split_proba * len(data))
            np.save(cls.data_dir / f"train_{seq_len}.npy", data[:nb_train])
            np.save(cls.data_dir / f"test_{seq_len}.npy", data[nb_train:])
            logger.debug(f"Sequences of length {seq_len} done. Saved in {cls.data_dir} ({nb_train}/{len(data)} split).")

    def load_data(self, lengths, data_type=None):
        """
        Get data (load from data directory).

        Parameters
        ----------
        lengths : list of int
            List of sequence lengths.
        data_type : str, optional
            Type of data to load. Whether 'train', 'test' or 'all'.

        Returns
        -------
        data : numpy.ndarray
            Data containing sequence of tokens with
                0: begining of sentence,
                1: end of input,
                2: end of sentence,
                x: other tokens generated by `generate_fixed_length_data`.
        indices : numpy.ndarray
            Indices to split the data by sequence length.

        Notes
        -----
        Should be called after `generate_datafiles`.
        """
        assert isinstance(lengths, list), "`lenghts` must be an a list of int."
        assert data_type in ["train", "test", "all"], "`data_type` must be 'train', 'test' or 'all'."

        prefix = data_type
        if data_type == "all":
            prefix = "data"

        # memory preallocation
        # ... compute the data size by lenghts
        nb_data_by_lens = np.empty(len(lengths))
        for i, seq_len in enumerate(lengths):
            filename = self.data_dir / f"{prefix}_{seq_len}.npy"
            with open(filename, "rb") as f:
                version = np.lib.format.read_magic(f)
                header = np.lib.format._read_array_header(f, version)
            nb_data_by_lens[i] = header[0][0]

        # ... deduce memory allocation
        indices = np.cumsum(nb_data_by_lens, dtype=int)
        indices = np.insert(indices, 0, 0)
        data = np.full((indices[-1], self.get_len(max(lengths)) + 2), 2, dtype=np.int32)
        data[:, 0] = 0

        # load the data in the allocated memory
        for i, seq_len in enumerate(lengths):
            data[indices[i] : indices[i + 1], 1 : self.get_len(seq_len) + 1] = np.load(
                self.data_dir / f"{prefix}_{seq_len}.npy"
            )

        return data, indices

    def set_data(self, lengths, data_type):
        """
        Load training data as a class attribute.

        Endows `self` with attributes `train_data` and `indices`.

        Parameters
        ----------
        lengths : list of int
            List of sequence lengths.
        data_type : str
            Type of data to load. Whether 'train', 'test' or 'all'.

        Notes
        -----
        Should be called after `generate_datafiles`.
        """
        train_data, indices = self.load_data(lengths, data_type=data_type)
        self.data = torch.from_numpy(train_data)
        self.indices = torch.from_numpy(indices)

    def set_data_probas(self, probas_by_len):
        """
        Set the probability of each data point.

        Endows `self` with attributes `proba_by_data`.

        Parameters
        ----------
        probas_by_len : list of numpy.ndarray
            Probability vector to sample of sequence of a given length.

        Notes
        -----
        Should be called after `set_train_data`.
        """

        assert abs(sum(probas_by_len) - 1) < 1e-6, "The sum of the probabilities must be equal to 1."

        self.probas = torch.empty(self.indices[-1])
        for i in range(len(probas_by_len)):
            start, end = self.indices[i], self.indices[i + 1]
            if start != end:
                self.probas[start:end] = probas_by_len[i] / (end - start)

    def set_as_trainset(self, lengths, probas_by_len):
        """
        Data generation processing.

        Parameters
        ----------
        lengths : list of int
            List of sequence lengths.
        probas_by_len : list of numpy.ndarray
            Probability vector to sample a sequence of a given length.
        rng : numpy.random.Generator, optional
            Random number generator. If None, use the default generator.
        """

        logger.info(f"Loading training data for {self.prefix} problem.")
        self.set_data(lengths, data_type="train")

        logger.info("Setting sampler.")
        self.set_data_probas(probas_by_len)
        self.sampler = WeightedRandomSampler(self.probas, len(self.data))

    def set_as_testset(self, lengths):
        """
        Data loading processing.

        Parameters
        ----------
        lengths : list of int
            List of sequence lengths.
        """

        logger.info(f"Loading test data for {self.prefix} problem.")
        self.set_data(lengths, data_type="test")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

    def eval_model(self, model, batch_size=None, special=False):
        """
        Eval model on dataset

        Parameters
        ----------
        model: torch.nn.Module
            model to be evaluated.
        dataset: torch.utils.data.Dataset
            dataset to evaluate the model.
        batch_size: int, optional
            batch size to use when data does not fit in memory.
        special: bool, optional (default is False)
            whether to compute special token syntaxic error.

        Returns
        -------
        err_by_len: torch.Tensor
            token errors average by lengths.
        seq_err_by_len: torch.Tensor
            sequence errors average by lengths.
        spe_err: torch.Tensor of size (-1, 3)
            sequence of special token erros.
        """

        if batch_size is None:
            batch_size = len(self.data)

        device = list(model.parameters())[0].device

        nb_data = len(self.data)
        err = torch.empty(nb_data, device=device, dtype=float)
        seq_err = torch.empty(nb_data, device=device, dtype=bool)
        if special:
            spe_err = torch.zeros(3, device=device, dtype=float)

        begin = 0
        for end in range(batch_size, nb_data + batch_size, batch_size):
            data = self.data[begin:end].to(device=device, dtype=torch.long)
            pred = model(data[:, :-1]).argmax(dim=-1)
            ground_truth = data[:, 1:]

            ind = ground_truth == 1
            cot_mask = ind.cumsum(axis=1)
            cot_mask[ind] = 0
            cot_mask = cot_mask.to(dtype=bool)
            pred[~cot_mask] = ground_truth[~cot_mask]

            errors = pred != ground_truth
            seq_err[begin:end] = errors.any(dim=1)
            err[begin:end] = errors.float().mean(dim=1)
            if special:
                tmp = self.eval_spe_tok_err(pred)
                spe_err[:] += torch.Tensor(tmp) * (end - begin)

            begin = end

        ind = self.indices
        err_by_len = err.cumsum(dim=0)[ind - 1]
        err_by_len[ind == 0] = 0
        err_by_len = err_by_len.diff()

        seq_err_by_len = seq_err.cumsum(dim=0)[ind - 1]
        seq_err_by_len[ind == 0] = 0
        seq_err_by_len = seq_err_by_len.diff().float()

        nb_by_len = ind.diff()
        nb_by_len[nb_by_len == 0] = 1
        err_by_len /= nb_by_len
        seq_err_by_len /= nb_by_len
        if special:
            spe_err /= end

            return err_by_len, seq_err_by_len, spe_err
        return err_by_len, seq_err_by_len

    @staticmethod
    def eval_spe_tok_err(pred):
        """
        Compute special token syntaxic error.

        Parameters
        ----------
        pred: torch.Tensor
            predictions of CoT with correct prefix.

        Returns
        -------
        bos_err: float
            number of `begin of sentence` syntaxic error.
        eoi_err: float
            number of `end of input` syntaxic error.
        eos_err: float
            number of `end of sentence` syntaxic error.
        """

        eos_ind = (pred == 2).int()
        first_eos = eos_ind.argmax(dim=-1)
        nb_eos = eos_ind.sum(dim=-1)

        eos_err = (first_eos + nb_eos) != 18
        bos_err = (pred == 0).int().sum(dim=-1) != 0
        eoi_err = (pred == 1).int().sum(dim=-1) != 1

        eos_err = eos_err.float().mean()
        bos_err = bos_err.float().mean()
        eoi_err = eoi_err.float().mean()

        return bos_err, eoi_err, eos_err


# -----------------------------------------------------------------------------
# Copy problem
# -----------------------------------------------------------------------------


class BinaryCopy(SequenceDataset):
    prefix = "binary_copy"
    data_dir = SequenceDataset.data_dir / prefix

    def __init__(self):
        super().__init__()

    @classmethod
    def generate_fixed_len_data(cls, seq_len, nb_data, rng=None):
        """
        Generate parity data with fixed sequence length.

        Parameters
        ----------
        seq_len : int
            Length of the sequence.
        nb_data : int
            Number of data points to generate.
            Will be reduced to 2**seq_len if greater.
        rng : numpy.random.Generator, optional
            Random number generator. If None, use the default generator.
            Used if nb_data is too small compared to all the potential sequences.

        Returns
        -------
        data: numpy.ndarray
            Generated data containing sequence of tokens with
                0: begining of sentence,
                1: end of input,
                2: end of sentence,
                3: negative bit,
                4: positive bit.
        """
        if rng is None:
            rng = np.random.default_rng()

        # allocate memory
        if 2**seq_len < nb_data:
            nb_data = 2**seq_len
        length = cls.get_len(seq_len)
        data = np.empty((nb_data, length), dtype=np.int32)

        # input data
        # ... exhaustive case
        if 2**seq_len == nb_data:
            powers_of_two = 2 ** np.arange(seq_len)[::-1]
            data[:, :seq_len] = (np.arange(nb_data).reshape(-1, 1) & powers_of_two != 0).astype(np.int32)
        # ... non-exhaustive case
        else:
            data[:, :seq_len] = (rng.random((nb_data, seq_len)) > 0.5).astype(np.int32)
        data += 3

        # end of input
        data[:, seq_len] = 1

        # copying the data
        data[:, seq_len + 1 :] = data[:, :seq_len]
        return data

    @classmethod
    def get_len(cls, seq_len):
        """Full sequence length."""
        return 2 * seq_len + 1


class Copy(SequenceDataset):
    prefix = "copy"
    data_dir = SequenceDataset.data_dir / prefix
    vocab_size = 10

    def __init__(self, vocab_size=None):
        super().__init__()
        if vocab_size is not None:
            Copy.vocab_size = vocab_size

    @classmethod
    def generate_fixed_len_data(cls, seq_len, nb_data, rng=None):
        """
        Generate parity data with fixed sequence length.

        Parameters
        ----------
        seq_len : int
            Length of the sequence.
        nb_data : int
            Number of data points to generate.
            Will be reduced to 2**seq_len if greater.
        rng : numpy.random.Generator, optional
            Random number generator. If None, use the default generator.
            Used if nb_data is too small compared to all the potential sequences.

        Returns
        -------
        data: numpy.ndarray
            Generated data containing sequence of tokens with
                0: begining of sentence,
                1: end of input,
                2: end of sentence,
                x between 3 and 2 + `vocab_size`: some tokens.
        """
        logger.info(f"Generating data with vocabulary of size {cls.vocab_size}.")

        if rng is None:
            rng = np.random.default_rng()

        # input
        length = cls.get_len(seq_len)
        data = np.empty((nb_data, length), dtype=np.int32)
        data[:, :seq_len] = (rng.random((nb_data, seq_len)) * cls.vocab_size).astype(np.int32)
        data += 3

        # end of input
        data[:, seq_len] = 1

        # copying the data
        data[:, seq_len + 1 :] = data[:, :seq_len]
        return data

    @classmethod
    def get_len(cls, seq_len):
        """Full sequence length."""
        return 2 * seq_len + 1


# -----------------------------------------------------------------------------
# Parity problem
# -----------------------------------------------------------------------------


class Parity(SequenceDataset):
    prefix = "parity"
    data_dir = SequenceDataset.data_dir / prefix

    def __init__(self):
        super().__init__()

    @classmethod
    def generate_fixed_len_data(cls, seq_len, nb_data, rng=None):
        """
        Generate parity data with fixed sequence length.

        Parameters
        ----------
        seq_len : int
            Length of the sequence.
        nb_data : int
            Number of data points to generate.
            Will be reduced to 2**seq_len if greater.
        rng : numpy.random.Generator, optional
            Random number generator. If None, use the default generator.
            Used if nb_data is too small compared to all the potential sequences.

        Returns
        -------
        data: numpy.ndarray
            Generated data containing sequence of tokens with
                0: begining of sentence,
                1: end of input,
                2: end of sentence,
                3: negative bit,
                4: positive bit.
        """
        if rng is None:
            rng = np.random.default_rng()

        # allocate memory
        if 2**seq_len < nb_data:
            nb_data = 2**seq_len
        length = cls.get_len(seq_len)
        data = np.empty((nb_data, length), dtype=np.int32)

        # input data
        # ... exhaustive case
        if 2**seq_len == nb_data:
            powers_of_two = 2 ** np.arange(seq_len)[::-1]
            data[:, :seq_len] = (np.arange(nb_data).reshape(-1, 1) & powers_of_two != 0).astype(np.int32)
        # ... non-exhaustive case
        else:
            data[:, :seq_len] = (rng.random((nb_data, seq_len)) > 0.5).astype(np.int32)

        # end of input
        data[:, seq_len] = -2

        # CoT data
        data[:, seq_len + 1 :] = np.cumsum(data[:, :seq_len], axis=1) % 2
        data += 3

        return data

    @classmethod
    def get_len(cls, seq_len):
        """Full sequence length."""
        return 2 * seq_len + 1
