# First experiments:
# Debugging configuration

launcher:
  name: utilization
  overwrite: true
  log_dir: $HOME/logs/exp0/utilization_small

  script: src.apps.gssm.train_onfly

  slurm:
    nodes: 1
    nb_gpus: 1        # use torchrun, or slurm, for multi-gpu runs
    time: 10          # job time in minutes
    signal_time: 60   # alert time in seconds

  grid:
    model:
      emb_dim: [32, 64, 128, 256]
      nb_heads: [2, 4, 8, 12]
      nb_layers: [2, 4, 8, 12]

    data:
      batch_size: [1, 2, 4, 8, 16, 32, 64]

run_config:
  cluster:
    compile_model: false  # You can compile on V100 or better GPU (I have P100, which do not allow for compilation)

  data:
    seq_len: 2048
    seed: 42
    batch_size: 16
    gssm:
      nodes:
      - name: X
        state_dim: 32
        parents: [X]
        alpha: 1e-1

  model:
    vocab_size: 32
    emb_dim: 256
    nb_layers: 12
    nb_heads: 12

  optim:
    steps: 20
    lr: 1e-2
    weight_decay: 0
    scheduler: constant

  orchestration:
    utils:
      seed: 100

    checkpoint:
      period: -1
      keep_only: 3

    logging:
      period: 1
      level: info

    profiler:
      active: true
      heavy: true
      wait: 1
      steps: 10

    wandb:
      active: false
