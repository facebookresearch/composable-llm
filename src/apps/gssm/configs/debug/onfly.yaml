# Debugging configuration

launcher:
  name: debug
  overwrite: true
  log_dir: $HOME/logs/debug_onfly

  script: src.apps.gssm.train_onfly

  slurm:
    nodes: 1
    nb_gpus: 2        # use torchrun, or slurm, for multi-gpu runs
    time: 10          # job time in minutes
    signal_time: 60   # alert time in seconds

run_config:
  cluster:
    compile_model: false  # You can compile on V100 or better GPU (I have P100, which do not allow for compilation)

  data:
    seq_len: 33
    batch_size: 16
    seed: 42
    gssm:
      nodes:
      - name: Z1
        state_dim: 256
        parents: []
        alpha: 1
      - name: Z2
        state_dim: 512
        parents: []
        alpha: 1
      - name: X
        state_dim: 256
        parents: [Z1, Z2]
        alpha: 1e-2
        observed: True

  model:
    emb_dim: 64
    nb_layers: 2
    block:
      nb_heads: 2
      hidden_dim: 256

    # implementation: mamba
    # emb_dim: 256
    # nb_layers: 2
    # block:
    #   nb_heads: 8
    #   state_dim: 64
    #   conv_size: 4

    # implementation: mingru
    # emb_dim: 256
    # nb_layers: 2
    # block:
    #   nb_heads: 8
    #   conv_size: null

  optim:
    steps: 100
    lr: 1e-2
    weight_decay: 0.1
    warmup: 10
    lr_min_ratio: 0

  orchestration:
    name: debug 
    utils:
      seed: 100

    checkpoint:
      period: 20
      keep_only: 3

    logging:
      period: 1
      level: debug

    profiler:
      active: true
      wait: 1
      steps: -1

    wandb:
      active: false
